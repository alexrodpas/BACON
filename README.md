# BACON
BACON: Deep-Learning Powered AI for Poetry Generation with Author Linguistic Style Transfer

_Note: code is right now under revision, it'll be uploaded shortly._

**BACON** (**B**asic **A**I for **C**ollaborative p**O**etry writi**N**g) is a project on _computational linguistics_ that I developed during my Junior high school year.

BACON is a prototype of an **automatic poetry generator with (a basic version of linguistic style transfer)**: it writes original, meaningful poetry in the style of any given author, with a quality and resemblance high enough as to be indistinguishable from the existing works of such author. The name is coined after Sir Francis Bacon who, according to some, was who actually wrote William Shakespeare’s plays.

The application of artificial neural networks to the problem of style transfer has been implemented successfully for paintings by Gatys, Ecker, and Bethge (2015). Linguistic style transfer for prose works has been recently explored for example in Ficler and Goldberg (2017), Xu et al. (2012).

BACON approaches the problem of automatic poetry generation with linguistic style transfer by splitting the solution in two components:

1. a **linguistic style modeler** (LSM), which builds a probabilistic model of the style used by any given author, and, 
1. a **deep-learning powered automatic poem generator** (APG) which uses the model generated by the LSM to guide the generation of original, meaningful poetry, with rich aesthetic rules, in the style of such author.

For the APG module BACON builds on the research developed by Hopkins and Kiela (2017). Their approach addresses the problem of automatically creating correct and meaningful poetry as “a constraint satisfaction problem imposed on the output of a generative model, where the constraints to restrict the types of generated poetry can be modified at will.” Their solution combines, in a pipeline, the following two components: 

1. a _generative language model_ representing content, which is implemented through a Long Short Term Memory (LSTM) Recurrent Neural Network (RNN), and
1. a _discriminative model_ representing form, which is implemented through a Weighted Finite State Transducer (WFST).

The LSM module generates a probabilistic model of a given author’s linguistic style by extracting high-entropy n-grams -through Term-Frequency-Inverse Document Frequency (TF-IDF)- and latent topics -through Latent Dirichlet Analysis (LDA)- from the author corpus, which is parsed against two Vector Space Models (VSM):

1. a large set of English poetry texts, consisting of 7.6 million words and 34.4 million characters, taken from 20th century poetry books, and 
1. a large set of general English language texts, consisting of a full English Wikipedia dump, consisting of 5.5 million documents and 43.6 million pages.

Linguistic style transfer is achieved by probabilistic conditioning/boosting of the high-entropy n-grams and topic words in the LSM applied to the APG module.

An extrinsic evaluation procedure was performed by conducting an indistinguishability study with a selection of poems written by a human poet and automatically generated poems in the style of that same author -a variant of the so-called Turing test for art works. The results show that participants were unable to tell the difference between human and BACON-generated poems in any statistically significant way.

**References**

Ficler, J., Goldberg, Y. (2017). _Controlling linguistic style aspects in neural language generation_. ArXiv Pre- print ArXiv:1707.02633.

Gatys, L. A., Ecker, A. S., Bethge, M. (2015). _A Neural Algorithm of Artistic Style_. ArXiv:1508.06576 [Cs,q-Bio].

Hopkins, J., Kiela, D. (2017). _Automatically Generating Rhythmic Verse with Neural Networks_. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Vol. 1, pp. 168–178).

Xu, W., Ritter, A., Dolan, B., Grishman, R., Cherry, C. (2012). _Paraphrasing for style_. Proceedings of COL- ING 2012, 2899–2914.
